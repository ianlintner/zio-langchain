package zio.langchain.evaluation

import zio.*
import zio.langchain.core.domain.*
import zio.langchain.core.model.LLM
import zio.langchain.core.retriever.Retriever
import zio.langchain.core.chain.Chain

import scala.collection.immutable.Map

/**
 * Core trait for all evaluators in ZIO LangChain.
 * Evaluators assess the quality and performance of LLM outputs, RAG systems, and other components.
 *
 * @tparam I The input type for the evaluation
 * @tparam O The output type being evaluated
 * @tparam R The result type of the evaluation
 */
trait Evaluator[-I, -O, +R]:
  /**
   * Evaluates the given input and output, producing an evaluation result.
   *
   * @param input The input that was provided to the system
   * @param output The output that was generated by the system
   * @return A ZIO effect that produces an evaluation result or fails with an EvaluationError
   */
  def evaluate(input: I, output: O): ZIO[Any, EvaluationError, R]

/**
 * Companion object for Evaluator.
 */
object Evaluator:
  /**
   * Creates a ZIO accessor for the Evaluator service.
   *
   * @return A ZIO effect that requires an Evaluator and produces the Evaluator
   */
  def get[I: izumi.reflect.Tag, O: izumi.reflect.Tag, R: izumi.reflect.Tag]: ZIO[Evaluator[I, O, R], Nothing, Evaluator[I, O, R]] =
    ZIO.service[Evaluator[I, O, R]]
  
  /**
   * Evaluates the given input and output using the Evaluator service.
   *
   * @param input The input that was provided to the system
   * @param output The output that was generated by the system
   * @return A ZIO effect that requires an Evaluator and produces an evaluation result or fails with an EvaluationError
   */
  def evaluate[I: izumi.reflect.Tag, O: izumi.reflect.Tag, R: izumi.reflect.Tag](input: I, output: O): ZIO[Evaluator[I, O, R], EvaluationError, R] =
    ZIO.serviceWithZIO[Evaluator[I, O, R]](_.evaluate(input, output))

/**
 * Represents the result of an evaluation.
 *
 * @param score The numerical score of the evaluation (typically 0.0 to 1.0)
 * @param metrics Additional metrics collected during evaluation
 * @param feedback Optional textual feedback explaining the evaluation
 */
case class EvaluationResult(
  score: Double,
  metrics: Map[String, Double] = Map.empty,
  feedback: Option[String] = None
):
  /**
   * Adds a metric to the evaluation result.
   *
   * @param name The name of the metric
   * @param value The value of the metric
   * @return A new EvaluationResult with the added metric
   */
  def withMetric(name: String, value: Double): EvaluationResult =
    copy(metrics = metrics + (name -> value))
  
  /**
   * Adds feedback to the evaluation result.
   *
   * @param feedback The feedback text
   * @return A new EvaluationResult with the added feedback
   */
  def withFeedback(feedback: String): EvaluationResult =
    copy(feedback = Some(feedback))

/**
 * Evaluator for RAG (Retrieval-Augmented Generation) systems.
 * Assesses the quality of retrieved documents and generated responses.
 */
trait RAGEvaluator extends Evaluator[String, (Seq[Document], String), EvaluationResult]:
  /**
   * Evaluates a RAG system's performance based on a query, retrieved documents, and generated response.
   *
   * @param query The user query
   * @param output A tuple containing (retrievedDocuments, generatedResponse)
   * @return A ZIO effect that produces an EvaluationResult or fails with an EvaluationError
   */
  def evaluate(query: String, output: (Seq[Document], String)): ZIO[Any, EvaluationError, EvaluationResult]

/**
 * Companion object for RAGEvaluator.
 */
object RAGEvaluator:
  /**
   * Creates a RAGEvaluator that assesses relevance, groundedness, and answer quality.
   *
   * @param llm The LLM to use for subjective assessments
   * @return A new RAGEvaluator
   */
  def make(llm: LLM): RAGEvaluator = new LLMBasedRAGEvaluator(llm)

/**
 * RAG evaluator that uses an LLM to assess the quality of retrieved documents and generated responses.
 *
 * @param llm The LLM to use for evaluation
 */
private class LLMBasedRAGEvaluator(llm: LLM) extends RAGEvaluator:
  override def evaluate(query: String, output: (Seq[Document], String)): ZIO[Any, EvaluationError, EvaluationResult] =
    val (retrievedDocs, generatedResponse) = output
    
    // Evaluate document relevance
    val relevanceEval = evaluateRelevance(query, retrievedDocs)
    
    // Evaluate response groundedness (factual accuracy based on retrieved docs)
    val groundednessEval = evaluateGroundedness(retrievedDocs, generatedResponse)
    
    // Evaluate answer quality
    val qualityEval = evaluateAnswerQuality(query, generatedResponse)
    
    // Combine all evaluations
    for
      relevance <- relevanceEval
      groundedness <- groundednessEval
      quality <- qualityEval
      
      // Calculate overall score (weighted average)
      overallScore = (relevance * 0.3) + (groundedness * 0.4) + (quality * 0.3)
    yield EvaluationResult(
      score = overallScore,
      metrics = Map(
        "relevance" -> relevance,
        "groundedness" -> groundedness,
        "answer_quality" -> quality
      )
    )
  
  /**
   * Evaluates the relevance of retrieved documents to the query.
   *
   * @param query The user query
   * @param docs The retrieved documents
   * @return A ZIO effect that produces a relevance score or fails with an EvaluationError
   */
  private def evaluateRelevance(query: String, docs: Seq[Document]): ZIO[Any, EvaluationError, Double] =
    if docs.isEmpty then ZIO.succeed(0.0)
    else
      val prompt = s"""
        |You are an objective evaluator assessing the relevance of retrieved documents to a query.
        |
        |Query: $query
        |
        |Retrieved Documents:
        |${docs.zipWithIndex.map { case (doc, i) => s"Document ${i + 1}:\n${doc.content}" }.mkString("\n\n")}
        |
        |On a scale from 0.0 to 1.0, how relevant are these documents to the query?
        |Consider:
        |1. Do the documents contain information that directly answers the query?
        |2. Are the documents on the same topic as the query?
        |3. Would these documents be useful for answering the query?
        |
        |Provide your assessment as a single number between 0.0 and 1.0, where:
        |0.0 = Completely irrelevant
        |1.0 = Perfectly relevant
        |
        |Score:
      """.stripMargin
      
      llm.complete(prompt)
        .map(parseScore)
        .mapError(e => EvaluationError(e, "Failed to evaluate document relevance"))
  
  /**
   * Evaluates the groundedness of the generated response in the retrieved documents.
   *
   * @param docs The retrieved documents
   * @param response The generated response
   * @return A ZIO effect that produces a groundedness score or fails with an EvaluationError
   */
  private def evaluateGroundedness(docs: Seq[Document], response: String): ZIO[Any, EvaluationError, Double] =
    if docs.isEmpty then ZIO.succeed(0.0)
    else
      val prompt = s"""
        |You are an objective evaluator assessing the groundedness of a response based on retrieved documents.
        |
        |Retrieved Documents:
        |${docs.zipWithIndex.map { case (doc, i) => s"Document ${i + 1}:\n${doc.content}" }.mkString("\n\n")}
        |
        |Generated Response:
        |$response
        |
        |On a scale from 0.0 to 1.0, how grounded is the response in the retrieved documents?
        |Consider:
        |1. Does the response contain claims that are not supported by the documents?
        |2. Does the response contradict information in the documents?
        |3. Does the response accurately represent the information in the documents?
        |
        |Provide your assessment as a single number between 0.0 and 1.0, where:
        |0.0 = Not grounded at all (contains unsupported claims or contradictions)
        |1.0 = Perfectly grounded (all information is supported by the documents)
        |
        |Score:
      """.stripMargin
      
      llm.complete(prompt)
        .map(parseScore)
        .mapError(e => EvaluationError(e, "Failed to evaluate response groundedness"))
  
  /**
   * Evaluates the overall quality of the generated response.
   *
   * @param query The user query
   * @param response The generated response
   * @return A ZIO effect that produces a quality score or fails with an EvaluationError
   */
  private def evaluateAnswerQuality(query: String, response: String): ZIO[Any, EvaluationError, Double] =
    val prompt = s"""
      |You are an objective evaluator assessing the quality of a response to a query.
      |
      |Query: $query
      |
      |Generated Response:
      |$response
      |
      |On a scale from 0.0 to 1.0, how would you rate the quality of this response?
      |Consider:
      |1. Does the response directly address the query?
      |2. Is the response clear, coherent, and well-structured?
      |3. Is the response comprehensive and informative?
      |4. Is the response concise without unnecessary information?
      |
      |Provide your assessment as a single number between 0.0 and 1.0, where:
      |0.0 = Very poor quality
      |1.0 = Excellent quality
      |
      |Score:
    """.stripMargin
    
    llm.complete(prompt)
      .map(parseScore)
      .mapError(e => EvaluationError(e, "Failed to evaluate answer quality"))
  
  /**
   * Parses a score from the LLM's response.
   *
   * @param response The LLM's response
   * @return The parsed score
   */
  private def parseScore(response: String): Double =
    val scorePattern = """(\d+\.\d+)""".r
    scorePattern.findFirstIn(response) match
      case Some(score) => score.toDouble.min(1.0).max(0.0)
      case None => 0.5 // Default to middle score if parsing fails

/**
 * Evaluator for chain outputs.
 * Assesses the quality of outputs from a Chain.
 *
 * @tparam I The input type for the chain
 * @tparam O The output type from the chain
 */
trait ChainEvaluator[I, O] extends Evaluator[I, O, EvaluationResult]:
  /**
   * Evaluates a chain's output based on the input and expected criteria.
   *
   * @param input The input provided to the chain
   * @param output The output generated by the chain
   * @return A ZIO effect that produces an EvaluationResult or fails with an EvaluationError
   */
  def evaluate(input: I, output: O): ZIO[Any, EvaluationError, EvaluationResult]

/**
 * Companion object for ChainEvaluator.
 */
object ChainEvaluator:
  /**
   * Creates a ChainEvaluator that uses an LLM to assess output quality.
   *
   * @param llm The LLM to use for evaluation
   * @param criteria The criteria to use for evaluation (e.g., "accuracy", "coherence", "relevance")
   * @param evaluationPrompt A function that generates an evaluation prompt from input and output
   * @return A new ChainEvaluator
   */
  def make[I, O](
    llm: LLM,
    criteria: Seq[String],
    evaluationPrompt: (I, O) => String
  ): ChainEvaluator[I, O] = new LLMBasedChainEvaluator(llm, criteria, evaluationPrompt)

/**
 * Chain evaluator that uses an LLM to assess the quality of chain outputs.
 *
 * @param llm The LLM to use for evaluation
 * @param criteria The criteria to use for evaluation
 * @param evaluationPrompt A function that generates an evaluation prompt from input and output
 * @tparam I The input type for the chain
 * @tparam O The output type from the chain
 */
private class LLMBasedChainEvaluator[I, O](
  llm: LLM,
  criteria: Seq[String],
  evaluationPrompt: (I, O) => String
) extends ChainEvaluator[I, O]:
  override def evaluate(input: I, output: O): ZIO[Any, EvaluationError, EvaluationResult] =
    val prompt = evaluationPrompt(input, output) + s"""
      |
      |Evaluate the output based on the following criteria:
      |${criteria.mkString("\n- ", "\n- ", "")}
      |
      |For each criterion, provide a score from 0.0 to 1.0, where:
      |0.0 = Does not meet the criterion at all
      |1.0 = Perfectly meets the criterion
      |
      |Format your response as:
      |Criterion1: score
      |Criterion2: score
      |...
      |Overall: overall_score
      |
      |Then provide a brief explanation of your evaluation.
    """.stripMargin
    
    llm.complete(prompt)
      .map(parseEvaluation)
      .mapError(e => EvaluationError(e, "Failed to evaluate chain output"))
  
  /**
   * Parses an evaluation from the LLM's response.
   *
   * @param response The LLM's response
   * @return The parsed EvaluationResult
   */
  private def parseEvaluation(response: String): EvaluationResult =
    val lines = response.split("\n").map(_.trim).filter(_.nonEmpty)
    
    // Extract scores for each criterion
    val criteriaScores = criteria.flatMap { criterion =>
      val pattern = s"$criterion:\\s*(\\d+\\.\\d+)".r
      pattern.findFirstMatchIn(response).map { m =>
        criterion.toLowerCase -> m.group(1).toDouble.min(1.0).max(0.0)
      }
    }.toMap
    
    // Extract overall score
    val overallPattern = """Overall:\s*(\d+\.\d+)""".r
    val overallScore = overallPattern.findFirstMatchIn(response)
      .map(_.group(1).toDouble.min(1.0).max(0.0))
      .getOrElse(criteriaScores.values.sum / criteriaScores.size.max(1))
    
    // Extract feedback (everything after the scores)
    val feedbackPattern = """(?i)(?:explanation|feedback):\s*(.+)""".r
    val feedback = feedbackPattern.findFirstMatchIn(response)
      .map(_.group(1).trim)
      .orElse(Some(lines.dropWhile(l => l.contains(":") && l.split(":").length >= 2).mkString(" ").trim))
      .filter(_.nonEmpty)
    
    EvaluationResult(
      score = overallScore,
      metrics = criteriaScores,
      feedback = feedback
    )

/**
 * Evaluator for comparing multiple system outputs.
 * Useful for A/B testing different models or configurations.
 *
 * @tparam I The input type
 * @tparam O The output type
 */
trait ComparativeEvaluator[I, O] extends Evaluator[I, Seq[O], Seq[EvaluationResult]]:
  /**
   * Evaluates and compares multiple outputs for the same input.
   *
   * @param input The input provided to the systems
   * @param outputs The outputs generated by different systems
   * @return A ZIO effect that produces a sequence of evaluation results or fails with an EvaluationError
   */
  def evaluate(input: I, outputs: Seq[O]): ZIO[Any, EvaluationError, Seq[EvaluationResult]]

/**
 * Companion object for ComparativeEvaluator.
 */
object ComparativeEvaluator:
  /**
   * Creates a ComparativeEvaluator that uses an LLM to compare outputs.
   *
   * @param llm The LLM to use for evaluation
   * @param criteria The criteria to use for evaluation
   * @return A new ComparativeEvaluator
   */
  def make[I, O](
    llm: LLM,
    criteria: Seq[String]
  ): ComparativeEvaluator[I, O] = new LLMBasedComparativeEvaluator(llm, criteria)

/**
 * Comparative evaluator that uses an LLM to compare multiple system outputs.
 *
 * @param llm The LLM to use for evaluation
 * @param criteria The criteria to use for evaluation
 * @tparam I The input type
 * @tparam O The output type
 */
private class LLMBasedComparativeEvaluator[I, O](
  llm: LLM,
  criteria: Seq[String]
) extends ComparativeEvaluator[I, O]:
  override def evaluate(input: I, outputs: Seq[O]): ZIO[Any, EvaluationError, Seq[EvaluationResult]] =
    if outputs.isEmpty then ZIO.succeed(Seq.empty)
    else if outputs.size == 1 then
      // If there's only one output, evaluate it directly
      val singleEvaluator = ChainEvaluator.make[I, O](
        llm,
        criteria,
        (i, o) => s"Input: $i\n\nOutput: $o"
      )
      singleEvaluator.evaluate(input, outputs.head).map(Seq(_))
    else
      // For multiple outputs, compare them
      val prompt = s"""
        |You are an objective evaluator comparing multiple system outputs for the same input.
        |
        |Input: $input
        |
        |${outputs.zipWithIndex.map { case (output, i) => s"Output ${i + 1}:\n$output" }.mkString("\n\n")}
        |
        |Evaluate each output based on the following criteria:
        |${criteria.mkString("\n- ", "\n- ", "")}
        |
        |For each output and criterion, provide a score from 0.0 to 1.0, where:
        |0.0 = Does not meet the criterion at all
        |1.0 = Perfectly meets the criterion
        |
        |Format your response as:
        |Output 1:
        |Criterion1: score
        |Criterion2: score
        |...
        |Overall: overall_score
        |
        |Output 2:
        |Criterion1: score
        |Criterion2: score
        |...
        |Overall: overall_score
        |
        |And so on for each output.
        |
        |Then provide a brief comparison of the outputs, highlighting strengths and weaknesses.
      """.stripMargin
      
      llm.complete(prompt)
        .map(parseComparison(_, outputs.size))
        .mapError(e => EvaluationError(e, "Failed to evaluate and compare outputs"))
  
  /**
   * Parses a comparison from the LLM's response.
   *
   * @param response The LLM's response
   * @param numOutputs The number of outputs being compared
   * @return The parsed sequence of EvaluationResults
   */
  private def parseComparison(response: String, numOutputs: Int): Seq[EvaluationResult] =
    val results = (1 to numOutputs).map { i =>
      val outputSection = s"Output $i:"
      val nextOutputSection = if i < numOutputs then s"Output ${i + 1}:" else "comparison"
      
      val sectionStart = response.indexOf(outputSection)
      val sectionEnd = response.toLowerCase.indexOf(nextOutputSection.toLowerCase, sectionStart)
      
      val section = if sectionStart >= 0 then
        if sectionEnd >= 0 then response.substring(sectionStart, sectionEnd)
        else response.substring(sectionStart)
      else ""
      
      // Extract scores for each criterion
      val criteriaScores = criteria.flatMap { criterion =>
        val pattern = s"$criterion:\\s*(\\d+\\.\\d+)".r
        pattern.findFirstMatchIn(section).map { m =>
          criterion.toLowerCase -> m.group(1).toDouble.min(1.0).max(0.0)
        }
      }.toMap
      
      // Extract overall score
      val overallPattern = """Overall:\s*(\d+\.\d+)""".r
      val overallScore = overallPattern.findFirstMatchIn(section)
        .map(_.group(1).toDouble.min(1.0).max(0.0))
        .getOrElse(criteriaScores.values.sum / criteriaScores.size.max(1))
      
      EvaluationResult(
        score = overallScore,
        metrics = criteriaScores
      )
    }
    
    // Extract the comparison section for feedback
    val comparisonStart = response.toLowerCase.indexOf("comparison")
    val feedback = if comparisonStart >= 0 then
      Some(response.substring(comparisonStart).trim)
    else None
    
    // Add the feedback to the first result
    if results.nonEmpty && feedback.isDefined then
      results.updated(0, results.head.withFeedback(feedback.get)) ++ results.tail
    else
      results

/**
 * Evaluator for assessing factual accuracy using a reference dataset.
 * Useful for evaluating question-answering systems against known ground truth.
 *
 * @tparam Q The question type
 * @tparam A The answer type
 */
trait FactualAccuracyEvaluator[Q, A] extends Evaluator[(Q, A), A, EvaluationResult]:
  /**
   * Evaluates the factual accuracy of a generated answer against a reference answer.
   *
   * @param input A tuple containing (question, referenceAnswer)
   * @param generatedAnswer The answer generated by the system
   * @return A ZIO effect that produces an EvaluationResult or fails with an EvaluationError
   */
  def evaluate(input: (Q, A), generatedAnswer: A): ZIO[Any, EvaluationError, EvaluationResult]

/**
 * Companion object for FactualAccuracyEvaluator.
 */
object FactualAccuracyEvaluator:
  /**
   * Creates a FactualAccuracyEvaluator that uses an LLM to assess factual accuracy.
   *
   * @param llm The LLM to use for evaluation
   * @return A new FactualAccuracyEvaluator
   */
  def make[Q, A](llm: LLM): FactualAccuracyEvaluator[Q, A] = new LLMBasedFactualAccuracyEvaluator(llm)

/**
 * Factual accuracy evaluator that uses an LLM to assess the accuracy of answers.
 *
 * @param llm The LLM to use for evaluation
 * @tparam Q The question type
 * @tparam A The answer type
 */
private class LLMBasedFactualAccuracyEvaluator[Q, A](llm: LLM) extends FactualAccuracyEvaluator[Q, A]:
  override def evaluate(input: (Q, A), generatedAnswer: A): ZIO[Any, EvaluationError, EvaluationResult] =
    val (question, referenceAnswer) = input
    
    val prompt = s"""
      |You are an objective evaluator assessing the factual accuracy of an answer.
      |
      |Question: $question
      |
      |Reference Answer: $referenceAnswer
      |
      |Generated Answer: $generatedAnswer
      |
      |On a scale from 0.0 to 1.0, how factually accurate is the generated answer compared to the reference answer?
      |Consider:
      |1. Does the generated answer contain the same key facts as the reference answer?
      |2. Does the generated answer contradict any facts in the reference answer?
      |3. Is the generated answer missing important information from the reference answer?
      |4. Does the generated answer include incorrect information not in the reference answer?
      |
      |Provide your assessment as a single number between 0.0 and 1.0, where:
      |0.0 = Completely inaccurate
      |1.0 = Perfectly accurate
      |
      |Then provide a brief explanation of your evaluation.
      |
      |Score:
    """.stripMargin
    
    llm.complete(prompt)
      .map { response =>
        val scorePattern = """(\d+\.\d+)""".r
        val score = scorePattern.findFirstIn(response)
          .map(_.toDouble.min(1.0).max(0.0))
          .getOrElse(0.5)
        
        val explanationPattern = """(?i)(?:explanation|feedback):\s*(.+)""".r
        val explanation = explanationPattern.findFirstMatchIn(response)
          .map(_.group(1).trim)
          .orElse {
            val lines = response.split("\n").map(_.trim).filter(_.nonEmpty)
            Some(lines.dropWhile(l => l.contains("Score:") || l.matches("""\d+\.\d+""")).mkString(" ").trim)
          }
          .filter(_.nonEmpty)
        
        EvaluationResult(
          score = score,
          metrics = Map("factual_accuracy" -> score),
          feedback = explanation
        )
      }
      .mapError(e => EvaluationError(e, "Failed to evaluate factual accuracy"))