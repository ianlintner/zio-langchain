package zio.langchain.evaluation

import zio.*
import zio.langchain.core.domain.*
import zio.langchain.core.model.LLM

import scala.collection.immutable.Map

/**
 * Specialized evaluator for RAG (Retrieval-Augmented Generation) metrics.
 * Provides detailed metrics for assessing RAG system performance.
 */
trait RAGMetricsEvaluator:
  /**
   * Evaluates a RAG system's performance with detailed metrics.
   *
   * @param query The user query
   * @param retrievedDocs The documents retrieved by the system
   * @param generatedAnswer The answer generated by the system
   * @param groundTruth Optional ground truth answer for comparison
   * @return A ZIO effect that produces a RAGMetricsResult or fails with an EvaluationError
   */
  def evaluate(
    query: String,
    retrievedDocs: Seq[Document],
    generatedAnswer: String,
    groundTruth: Option[String] = None
  ): ZIO[Any, EvaluationError, RAGMetricsResult]

/**
 * Companion object for RAGMetricsEvaluator.
 */
object RAGMetricsEvaluator:
  /**
   * Creates a RAGMetricsEvaluator that uses an LLM to assess various RAG metrics.
   *
   * @param llm The LLM to use for evaluation
   * @return A new RAGMetricsEvaluator
   */
  def make(llm: LLM): RAGMetricsEvaluator = new LLMBasedRAGMetricsEvaluator(llm)

/**
 * Represents the result of a RAG metrics evaluation.
 *
 * @param contextRelevance Score for how relevant the retrieved documents are to the query (0.0-1.0)
 * @param answerRelevance Score for how relevant the generated answer is to the query (0.0-1.0)
 * @param faithfulness Score for how faithful the generated answer is to the retrieved documents (0.0-1.0)
 * @param comprehensiveness Score for how comprehensive the answer is (0.0-1.0)
 * @param groundedness Score for how well-grounded the answer is in facts (0.0-1.0)
 * @param harmoniousScore A harmonic mean of the key metrics
 * @param additionalMetrics Any additional metrics collected during evaluation
 * @param feedback Optional textual feedback explaining the evaluation
 */
case class RAGMetricsResult(
  contextRelevance: Double,
  answerRelevance: Double,
  faithfulness: Double,
  comprehensiveness: Double,
  groundedness: Double,
  harmoniousScore: Double,
  additionalMetrics: Map[String, Double] = Map.empty,
  feedback: Option[String] = None
):
  /**
   * Converts this RAGMetricsResult to a standard EvaluationResult.
   *
   * @return An EvaluationResult with the same metrics
   */
  def toEvaluationResult: EvaluationResult =
    EvaluationResult(
      score = harmoniousScore,
      metrics = Map(
        "context_relevance" -> contextRelevance,
        "answer_relevance" -> answerRelevance,
        "faithfulness" -> faithfulness,
        "comprehensiveness" -> comprehensiveness,
        "groundedness" -> groundedness
      ) ++ additionalMetrics,
      feedback = feedback
    )

/**
 * RAG metrics evaluator that uses an LLM to assess various aspects of RAG system performance.
 *
 * @param llm The LLM to use for evaluation
 */
private class LLMBasedRAGMetricsEvaluator(llm: LLM) extends RAGMetricsEvaluator:
  override def evaluate(
    query: String,
    retrievedDocs: Seq[Document],
    generatedAnswer: String,
    groundTruth: Option[String] = None
  ): ZIO[Any, EvaluationError, RAGMetricsResult] =
    // Evaluate each metric in parallel
    val contextRelevanceEval = evaluateContextRelevance(query, retrievedDocs)
    val answerRelevanceEval = evaluateAnswerRelevance(query, generatedAnswer)
    val faithfulnessEval = evaluateFaithfulness(retrievedDocs, generatedAnswer)
    val comprehensivenessEval = evaluateComprehensiveness(query, generatedAnswer)
    val groundednessEval = evaluateGroundedness(retrievedDocs, generatedAnswer, groundTruth)
    
    // If ground truth is provided, also evaluate answer correctness
    val correctnessEval = groundTruth match
      case Some(truth) => evaluateCorrectness(generatedAnswer, truth)
      case None => ZIO.succeed(None)
    
    // Combine all evaluations
    for
      contextRelevance <- contextRelevanceEval
      answerRelevance <- answerRelevanceEval
      faithfulness <- faithfulnessEval
      comprehensiveness <- comprehensivenessEval
      groundedness <- groundednessEval
      correctness <- correctnessEval
      
      // Calculate harmonic mean of key metrics
      keyMetrics = Seq(contextRelevance, answerRelevance, faithfulness, groundedness)
      harmoniousScore = if keyMetrics.exists(_ == 0.0) then 0.0
                        else keyMetrics.size.toDouble / keyMetrics.map(1.0 / _).sum
      
      // Additional metrics
      additionalMetrics = correctness.map(c => Map("correctness" -> c)).getOrElse(Map.empty)
    yield RAGMetricsResult(
      contextRelevance = contextRelevance,
      answerRelevance = answerRelevance,
      faithfulness = faithfulness,
      comprehensiveness = comprehensiveness,
      groundedness = groundedness,
      harmoniousScore = harmoniousScore,
      additionalMetrics = additionalMetrics
    )
  
  /**
   * Evaluates the relevance of retrieved documents to the query.
   *
   * @param query The user query
   * @param docs The retrieved documents
   * @return A ZIO effect that produces a relevance score or fails with an EvaluationError
   */
  private def evaluateContextRelevance(query: String, docs: Seq[Document]): ZIO[Any, EvaluationError, Double] =
    if docs.isEmpty then ZIO.succeed(0.0)
    else
      val prompt = s"""
        |You are an objective evaluator assessing the relevance of retrieved documents to a query.
        |
        |Query: $query
        |
        |Retrieved Documents:
        |${docs.zipWithIndex.map { case (doc, i) => s"Document ${i + 1}:\n${doc.content}" }.mkString("\n\n")}
        |
        |On a scale from 0.0 to 1.0, how relevant are these documents to the query?
        |Consider:
        |1. Do the documents contain information that directly answers the query?
        |2. Are the documents on the same topic as the query?
        |3. Would these documents be useful for answering the query?
        |
        |Provide your assessment as a single number between 0.0 and 1.0, where:
        |0.0 = Completely irrelevant
        |1.0 = Perfectly relevant
        |
        |Score:
      """.stripMargin
      
      llm.complete(prompt)
        .map(parseScore)
        .mapError(e => EvaluationError(e, "Failed to evaluate context relevance"))
  
  /**
   * Evaluates the relevance of the generated answer to the query.
   *
   * @param query The user query
   * @param answer The generated answer
   * @return A ZIO effect that produces a relevance score or fails with an EvaluationError
   */
  private def evaluateAnswerRelevance(query: String, answer: String): ZIO[Any, EvaluationError, Double] =
    val prompt = s"""
      |You are an objective evaluator assessing the relevance of an answer to a query.
      |
      |Query: $query
      |
      |Generated Answer:
      |$answer
      |
      |On a scale from 0.0 to 1.0, how relevant is this answer to the query?
      |Consider:
      |1. Does the answer directly address the query?
      |2. Does the answer stay on topic?
      |3. Does the answer provide information that the user was looking for?
      |
      |Provide your assessment as a single number between 0.0 and 1.0, where:
      |0.0 = Completely irrelevant
      |1.0 = Perfectly relevant
      |
      |Score:
    """.stripMargin
    
    llm.complete(prompt)
      .map(parseScore)
      .mapError(e => EvaluationError(e, "Failed to evaluate answer relevance"))
  
  /**
   * Evaluates the faithfulness of the generated answer to the retrieved documents.
   * Faithfulness measures whether the answer contains only information that is present in the documents.
   *
   * @param docs The retrieved documents
   * @param answer The generated answer
   * @return A ZIO effect that produces a faithfulness score or fails with an EvaluationError
   */
  private def evaluateFaithfulness(docs: Seq[Document], answer: String): ZIO[Any, EvaluationError, Double] =
    if docs.isEmpty then ZIO.succeed(0.0)
    else
      val prompt = s"""
        |You are an objective evaluator assessing the faithfulness of an answer to the retrieved documents.
        |
        |Retrieved Documents:
        |${docs.zipWithIndex.map { case (doc, i) => s"Document ${i + 1}:\n${doc.content}" }.mkString("\n\n")}
        |
        |Generated Answer:
        |$answer
        |
        |On a scale from 0.0 to 1.0, how faithful is the answer to the retrieved documents?
        |Faithfulness means the answer contains ONLY information that is present in or can be directly inferred from the documents.
        |
        |Consider:
        |1. Does the answer contain any claims that are not supported by the documents?
        |2. Does the answer contradict any information in the documents?
        |3. Does the answer include speculations or assumptions not evidenced in the documents?
        |
        |Provide your assessment as a single number between 0.0 and 1.0, where:
        |0.0 = Not faithful at all (contains many unsupported claims)
        |1.0 = Perfectly faithful (contains only information from the documents)
        |
        |Score:
      """.stripMargin
      
      llm.complete(prompt)
        .map(parseScore)
        .mapError(e => EvaluationError(e, "Failed to evaluate faithfulness"))
  
  /**
   * Evaluates the comprehensiveness of the generated answer.
   * Comprehensiveness measures whether the answer covers all aspects of the query.
   *
   * @param query The user query
   * @param answer The generated answer
   * @return A ZIO effect that produces a comprehensiveness score or fails with an EvaluationError
   */
  private def evaluateComprehensiveness(query: String, answer: String): ZIO[Any, EvaluationError, Double] =
    val prompt = s"""
      |You are an objective evaluator assessing the comprehensiveness of an answer to a query.
      |
      |Query: $query
      |
      |Generated Answer:
      |$answer
      |
      |On a scale from 0.0 to 1.0, how comprehensive is this answer?
      |Comprehensiveness means the answer covers all aspects of the query and provides complete information.
      |
      |Consider:
      |1. Does the answer address all parts of the query?
      |2. Does the answer provide sufficient detail?
      |3. Are there important aspects of the query that the answer fails to address?
      |
      |Provide your assessment as a single number between 0.0 and 1.0, where:
      |0.0 = Not comprehensive at all (misses many important aspects)
      |1.0 = Perfectly comprehensive (covers all aspects thoroughly)
      |
      |Score:
    """.stripMargin
    
    llm.complete(prompt)
      .map(parseScore)
      .mapError(e => EvaluationError(e, "Failed to evaluate comprehensiveness"))
  
  /**
   * Evaluates the groundedness of the generated answer.
   * Groundedness measures whether the answer is based on factual information.
   *
   * @param docs The retrieved documents
   * @param answer The generated answer
   * @param groundTruth Optional ground truth answer for comparison
   * @return A ZIO effect that produces a groundedness score or fails with an EvaluationError
   */
  private def evaluateGroundedness(
    docs: Seq[Document],
    answer: String,
    groundTruth: Option[String]
  ): ZIO[Any, EvaluationError, Double] =
    val prompt = s"""
      |You are an objective evaluator assessing the groundedness of an answer.
      |
      |Retrieved Documents:
      |${docs.zipWithIndex.map { case (doc, i) => s"Document ${i + 1}:\n${doc.content}" }.mkString("\n\n")}
      |
      |Generated Answer:
      |$answer
      |
      |${groundTruth.map(gt => s"Ground Truth Answer:\n$gt").getOrElse("")}
      |
      |On a scale from 0.0 to 1.0, how grounded is this answer in factual information?
      |Groundedness means the answer is based on facts rather than opinions, speculations, or hallucinations.
      |
      |Consider:
      |1. Does the answer contain factual claims that can be verified?
      |2. Does the answer avoid speculations or unsubstantiated claims?
      |3. Is the answer consistent with the retrieved documents and ${groundTruth.map(_ => "ground truth").getOrElse("general knowledge")}?
      |
      |Provide your assessment as a single number between 0.0 and 1.0, where:
      |0.0 = Not grounded at all (mostly speculative or incorrect)
      |1.0 = Perfectly grounded (entirely factual and accurate)
      |
      |Score:
    """.stripMargin
    
    llm.complete(prompt)
      .map(parseScore)
      .mapError(e => EvaluationError(e, "Failed to evaluate groundedness"))
  
  /**
   * Evaluates the correctness of the generated answer against a ground truth.
   *
   * @param answer The generated answer
   * @param groundTruth The ground truth answer
   * @return A ZIO effect that produces an optional correctness score or fails with an EvaluationError
   */
  private def evaluateCorrectness(answer: String, groundTruth: String): ZIO[Any, EvaluationError, Option[Double]] =
    val prompt = s"""
      |You are an objective evaluator assessing the correctness of an answer against a ground truth.
      |
      |Generated Answer:
      |$answer
      |
      |Ground Truth Answer:
      |$groundTruth
      |
      |On a scale from 0.0 to 1.0, how correct is the generated answer compared to the ground truth?
      |
      |Consider:
      |1. Does the generated answer contain the same key facts as the ground truth?
      |2. Does the generated answer contradict any facts in the ground truth?
      |3. Is the generated answer missing important information from the ground truth?
      |
      |Provide your assessment as a single number between 0.0 and 1.0, where:
      |0.0 = Completely incorrect
      |1.0 = Perfectly correct
      |
      |Score:
    """.stripMargin
    
    llm.complete(prompt)
      .map(response => Some(parseScore(response)))
      .mapError(e => EvaluationError(e, "Failed to evaluate correctness"))
  
  /**
   * Parses a score from the LLM's response.
   *
   * @param response The LLM's response
   * @return The parsed score
   */
  private def parseScore(response: String): Double =
    val scorePattern = """(\d+\.\d+)""".r
    scorePattern.findFirstIn(response) match
      case Some(score) => score.toDouble.min(1.0).max(0.0)
      case None => 0.5 // Default to middle score if parsing fails